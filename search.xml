<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[The Josephus Problem]]></title>
    <url>%2F2018%2F09%2F09%2FThe-Josephus-Problem%2F</url>
    <content type="text"><![CDATA[简介约瑟夫问题： 据说著名犹太历史学家 Josephus有过以下的故事：在罗马人占领乔塔帕特后，39 个犹太人与Josephus及他的朋友躲到一个洞中，39个犹太人决定宁愿死也不要被敌人抓到，于是决定了一个自杀方式，41个人排成一个圆圈，由第1个人开始报数，每报数到第3人该人就必须自杀，然后再由下一个重新报数，直到所有人都自杀身亡为止。然而Josephus 和他的朋友并不想遵从。首先从一个人开始，越过k-2个人（因为第一个人已经被越过），并杀掉第k个人。接着，再越过k-1个人，并杀掉第k个人。这个过程沿着圆圈一直进行，直到最终只剩下一个人留下，这个人就可以继续活着。问题是，给定了和，一开始要站在什么地方才能避免被处决？Josephus要他的朋友先假装遵从，他将朋友与自己安排在第16个与第31个位置，于是逃过了这场死亡游戏。 现在我们将问题一般化，即问题为： n个人围成一圈，从围成标记号为1到n的圆圈的n个人开始，每隔一个删去一个人，直到只有一个人幸存下来。求确定幸存者的号码 J(n) 解法不妨从奇偶性开始考虑当n为偶数时,我们考虑 J(2n) 的情况。第一轮后，还剩下n个人。此时发现编号符合映射：x \rightarrow y = 2x-1,可得到J(2n) = 2J(n)-1同理，当n为奇数时]]></content>
      <categories>
        <category>Mathematics</category>
        <category>Concrete Mathematics</category>
      </categories>
      <tags>
        <tag>Concrete Mathematics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K-NearestNeighbor]]></title>
    <url>%2F2018%2F08%2F20%2Fk-NearestNeighbor%2F</url>
    <content type="text"><![CDATA[简介KNN算法是测量不同特征值之间的距离方法进行分类和回归的非参数统计方法，属于懒惰学习(lazy learning)中的一种。 在k-NN分类中，输出是一个分类族群。一个对象的分类是由其邻居的“多数表决”确定的，k个最近邻居（k为正整数，通常较小）中最常见的分类决定了赋予该对象的类别。若 k = 1，则该对象的类别直接由最近的一个节点赋予。 在k-NN回归中，输出是该对象的属性值。该值是其k个最近邻居的值的平均值。 优点 精度高 对异常值不敏感 无数据输入假定 缺点 计算复杂度高、空间复杂度高。时间复杂度为 $O(n)$ 适用范围 该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。 数值型和标称型数据 注： 标称型：一般在有限的数据中取，而且只存在‘是’和‘否’两种不同的结果（一般用于分类）数值型：可以在无限的数据中取，而且数值比较具体化，例如4.02,6.23这种值（一般用于回归分析） 分类算法流程计算已知数据集中的点与当前点的距离12def classifyKNN(inX, dataSet, labels, k): diffMat = (np.tile(inx,(dataSet.shape[0],1)) - dataSet) ** 2 按照距离递增次序排序注: 这里使用欧式距离 12distance = (diffMat.sum(axis = 1)) ** 0.5sortedDistIndicies = distances.argsort() 注： x.argsort: 将x中的元素从小到大排列，提取其对应的index(索引)，然后输出到y。如 x = np.array([1,4,3,-1]), 则输出 array([3, 0, 2, 1]) 选取与当前点距离最小的k个点并统计类别频率12345classCount = &#123;&#125;for i in range(k): voteIlabel = labels[sortedDistIndicies[i]] #if label not in classCount,set 0,else it's count plus 1 classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 返回前k个点频率最高的类别作为当前点的预测分类12sortedClassCount = sorted(classCount.iteritems(),key = operator.itemgetter(1), reverse = True)return sortedClassCount[0][0] 注: 迭代大数据字典时，如果是使用 items() 方法，那么在迭代之前，迭代器迭代前需要把数据完整地加载到内存，则会造成内存占用大。而 iteritem() 返回一个迭代器(iterators)，迭代器在迭代的时候，迭代元素逐个生成，减少了内存消耗。 分类完整代码123456789101112131415import numpy as npimport pandas as pdimport operator def KNNclassify(inx,dataSet,labels,k): diffMat = np.tile(inx, (dataSet.shape[0], 1)) - dataSet distances = ((diffMat**2).sum(axis = 1))**0.5 sortedDistIndicies = distances.argsort() classCount = &#123;&#125; for i in range(k): voteIlabel = labels[sortedDistIndicies[i]] #if voteIlabel not in classCount,set 0,else count plus 1 classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse = True) return sortedClassCount[0][0]]]></content>
      <categories>
        <category>Machine Learning</category>
        <category>Stat ML</category>
      </categories>
      <tags>
        <tag>Stat ML</tag>
      </tags>
  </entry>
</search>
