<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[The Josephus Problem]]></title>
    <url>%2F2018%2F09%2F09%2FThe-Josephus-Problem%2F</url>
    <content type="text"><![CDATA[简介约瑟夫问题： 据说著名犹太历史学家 Josephus有过以下的故事：在罗马人占领乔塔帕特后，39 个犹太人与Josephus及他的朋友躲到一个洞中，39个犹太人决定宁愿死也不要被敌人抓到，于是决定了一个自杀方式，41个人排成一个圆圈，由第1个人开始报数，每报数到第3人该人就必须自杀，然后再由下一个重新报数，直到所有人都自杀身亡为止。然而Josephus 和他的朋友并不想遵从。首先从一个人开始，越过k-2个人（因为第一个人已经被越过），并杀掉第k个人。接着，再越过k-1个人，并杀掉第k个人。这个过程沿着圆圈一直进行，直到最终只剩下一个人留下，这个人就可以继续活着。问题是，给定了和，一开始要站在什么地方才能避免被处决？Josephus要他的朋友先假装遵从，他将朋友与自己安排在第16个与第31个位置，于是逃过了这场死亡游戏。现在我们将问题一般化，即问题为： n个人围成一圈，从围成标记号为1到n的圆圈的n个人开始，每隔一个删去一个人，直到只有一个人幸存下来。求确定幸存者的号码 $J(n)$ 解法递归不妨从奇偶性开始考虑当$n$为偶数时，我们考虑 $J(2n)$ 的情况。第一轮后，还剩下n个人。此时发现编号符合映射: $x \rightarrow y = 2x-1$,可得到 $J(2n) = 2J(n)-1$同理，当 $n$ 为奇数时,我们考虑 $J(2n+1)$ 的情况，此时可得：$J(2n+1)=2J(n)+1$ 故定义$J$的递归式为： J(1) = 1J(2n) = 2J(n)-1, n\geq1J(2n+1) = 2J(n) + 1,n\geq1封闭形式通过递归式，我们可以观察解从而将递归式转化为封闭形式的解。 $n$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $J(n)$ 1 1 3 1 3 5 7 1 3 5 7 9 11 13 15 1 通过观察，如果我们将 $n$ 写成 $n=2^m+l$ ,则递归式解为： J(2^m+l) = 2l+1, m\geq0, 0\leq l < 2^m.约束条件为： 2^m\leq n\leq 2^{m+1}证明可用数学归纳法证明，此处略。 进制通过求解封闭形式，我们发现2的幂起着重要作用。于是研究以2为基数的表示方法。假设n的二进制展开式为 ${(b{_m}b_{m-1}b_{m-2}\cdots b_1b_0)}_2$ ，$b$ 取0或1，$b_m$=1.我们依次可得： n={(1b_{m-1}b_{m-2}\cdots b_1b_0)}_2l={(0b_{m-1}b_{m-2}\cdots b_1b_0)}_22l={(b_{m-1}b_{m-2}\cdots b_1b_00)}_22l+1={(b_{m-1}b_{m-2}\cdots b_1b_01)}_2J(n)={(b_{m-1}b_{m-2}\cdots b_1b_0b_m)}_2二进制下即可数位循环移动得到最终解。 扩展不动点若对 $J(n)$ 不断迭代，则最后会达到一个不动点，在该点有 $J(n)=n$.设 $J(n)$ 二进制中有k个1,则不断迭代后，$n=2^{k}-1$. 递归式通解f(1) = \alphaf(2n) = 2f(n) + \beta ,n \geq 1f(2n+1) = 2f(n) + \gamma ,n \geq 1对n的前几项列出表格 $n$ 1 2 3 4 5 6 $J(n)$ $\alpha$ $2\alpha+\beta+0\gamma$ $2\alpha+0\beta+1\gamma$ $4\alpha+3\beta+0\gamma$ $4\alpha+2\beta+1\gamma$ $4\alpha+1\beta+2\gamma$ 将 $f(n)$ 与 $\alpha,\beta,\gamma$ 的依存关系分离开来，表示为： f(n)=A(n)\alpha+B(n)\beta+C(n)\gamma可通过归纳法证得： A(n) = 2^mB(n) = 2^m-1-lC(n) = l其中有 $n=2^m+l$ 以及 $0\leq l&lt;2^m$ $(n\geq 1)$ 成套解法(repertoire method)从 $f(n)$ 出发，并研究是否有任意常数 $(\alpha,\beta,\gamma)$ 能定义它。 当 $f(n)=1$，可得解 $(\alpha,\beta,\gamma)=(1,-1,-1)$ , $A(n)-B(n)-C(n)=f(n)=1$同理当$ f(n)=n$ 时， $(\alpha,\beta,\gamma)=(1,0,1)$ 时对所有的n成立，故$ A(n)+C(n)=f(n)=n$ 可得： A(n) = 2^mA(n)-B(n)-C(n)=1A(n)+C(n)=n解同上. 进制通解令 $\beta_0 = \beta$ 以及 $\beta_1 = \gamma$，则推广递归式改写成 f(1) = \alphaf(2n+j) = 2f(n) + \beta_ j, j = 0,1,n\geq 1递归式由上两式按二进制展开为： f(n) = f((b_mb_{m-1}\cdots b_1b_0)_2) = 2^m\alpha +2^{m-1}\beta_{b_{m-1}}+\cdots+2\beta_{b_1}+\beta_{b_0}2^m\alpha +2^{m-1}\beta_{b_{m-1}}+\cdots+2\beta_{b_1}+\beta_{b_0} =(\alpha\beta_{b_{m-1}}\beta_{b_{m-2}}\cdots \beta_{b_1}\beta_{b_0})_2即 f(n)=(\alpha\beta_{b_{m-1}}\beta_{b_{m-2}}\cdots \beta_{b_1}\beta_{b_0})_2求解结束。以下给出例子$\alpha =1,\beta=-1,\gamma=1$,此时 $n=(1100100)_2 = 100$$f(n) = (1$ $ 1-1-1$ $1-1-1)_2 = +64 +32-16-8+4-2-1=73$]]></content>
      <categories>
        <category>Mathematics</category>
        <category>Concrete Mathematics</category>
      </categories>
      <tags>
        <tag>Concrete Mathematics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K-NearestNeighbor]]></title>
    <url>%2F2018%2F08%2F20%2Fk-NearestNeighbor%2F</url>
    <content type="text"><![CDATA[简介KNN算法是测量不同特征值之间的距离方法进行分类和回归的非参数统计方法，属于懒惰学习(lazy learning)中的一种。 在k-NN分类中，输出是一个分类族群。一个对象的分类是由其邻居的“多数表决”确定的，k个最近邻居（k为正整数，通常较小）中最常见的分类决定了赋予该对象的类别。若 k = 1，则该对象的类别直接由最近的一个节点赋予。 在k-NN回归中，输出是该对象的属性值。该值是其k个最近邻居的值的平均值。 优点 精度高 对异常值不敏感 无数据输入假定 缺点 计算复杂度高、空间复杂度高。时间复杂度为 $O(n)$ 适用范围 该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。 数值型和标称型数据 注： 标称型：一般在有限的数据中取，而且只存在‘是’和‘否’两种不同的结果（一般用于分类）数值型：可以在无限的数据中取，而且数值比较具体化，例如4.02,6.23这种值（一般用于回归分析） 分类算法流程计算已知数据集中的点与当前点的距离12def classifyKNN(inX, dataSet, labels, k): diffMat = (np.tile(inx,(dataSet.shape[0],1)) - dataSet) ** 2 按照距离递增次序排序注: 这里使用欧式距离 12distance = (diffMat.sum(axis = 1)) ** 0.5sortedDistIndicies = distances.argsort() 注： x.argsort: 将x中的元素从小到大排列，提取其对应的index(索引)，然后输出到y。如 x = np.array([1,4,3,-1]), 则输出 array([3, 0, 2, 1]) 选取与当前点距离最小的k个点并统计类别频率12345classCount = &#123;&#125;for i in range(k): voteIlabel = labels[sortedDistIndicies[i]] #if label not in classCount,set 0,else it's count plus 1 classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 返回前k个点频率最高的类别作为当前点的预测分类12sortedClassCount = sorted(classCount.iteritems(),key = operator.itemgetter(1), reverse = True)return sortedClassCount[0][0] 注: 迭代大数据字典时，如果是使用 items() 方法，那么在迭代之前，迭代器迭代前需要把数据完整地加载到内存，则会造成内存占用大。而 iteritem() 返回一个迭代器(iterators)，迭代器在迭代的时候，迭代元素逐个生成，减少了内存消耗。 分类完整代码123456789101112131415import numpy as npimport pandas as pdimport operator def KNNclassify(inx,dataSet,labels,k): diffMat = np.tile(inx, (dataSet.shape[0], 1)) - dataSet distances = ((diffMat**2).sum(axis = 1))**0.5 sortedDistIndicies = distances.argsort() classCount = &#123;&#125; for i in range(k): voteIlabel = labels[sortedDistIndicies[i]] #if voteIlabel not in classCount,set 0,else count plus 1 classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse = True) return sortedClassCount[0][0]]]></content>
      <categories>
        <category>Machine Learning</category>
        <category>Stat ML</category>
      </categories>
      <tags>
        <tag>Stat ML</tag>
      </tags>
  </entry>
</search>
